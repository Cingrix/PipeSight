# ğŸ§  Automated Defect Detection using Image Processing & Deep Learning  

## ğŸ“– Overview
**PipeSight** is an **AI-powered defect detection system** designed to identify **leaks and defects in industrial pipelines** using **X-ray images**.  
This project integrates a trained deep learning model with a modern, responsive frontend to provide a seamless interface for defect inspection and classification.

---

## âš™ï¸ Tech Stack

### ğŸ§© Backend
- **FastAPI** â€“ Lightweight Python web framework for serving the ML model  
- **PyTorch** â€“ Deep learning framework used for model training and inference  
- **Pillow (PIL)** â€“ Image preprocessing and manipulation  
- **NumPy** â€“ Numerical and matrix operations  
- **CORS Middleware** â€“ Enables secure communication between backend and frontend  

### ğŸ’» Frontend
- **React + TypeScript (Vite)** â€“ High-performance frontend framework  
- **Tailwind CSS** â€“ Utility-first CSS framework for building responsive UIs  
- **shadcn/ui & Radix UI** â€“ Accessible and elegant component libraries  
- **Bun / Node.js** â€“ For package management and build processes  

---

## ğŸ—‚ï¸ Project Structure

```plaintext
pipe-sight-main/
â”‚
â”œâ”€â”€ backend/                  
â”‚   â”œâ”€â”€ main.py               # FastAPI backend (model API)
â”‚   â”œâ”€â”€ train.py              # Model training script
â”‚   â”œâ”€â”€ package.json          # Backend dependencies (if Node utilities used)
â”‚   â””â”€â”€ defect_classifier_retrained.pth  # Trained PyTorch model
â”‚
â”œâ”€â”€ frontend/                 
â”‚   â”œâ”€â”€ src/                  # React + TypeScript source code
â”‚   â”œâ”€â”€ public/               # Static assets
â”‚   â”œâ”€â”€ tailwind.config.ts    # Tailwind CSS configuration
â”‚   â”œâ”€â”€ vite.config.ts        # Vite configuration
â”‚   â””â”€â”€ package.json          # Frontend dependencies
â”‚
â””â”€â”€ README.md
```
ğŸš€ How to Run the Project
1ï¸âƒ£ Clone the Repository
```plaintext
git clone https://github.com/<your-username>/pipe-sight.git
cd pipe-sight-main
```
2ï¸âƒ£ Setup and Run the Backend
```plaintext
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

3ï¸âƒ£ Setup and Run the Frontend
```plaintext
cd ../frontend
npm install
npm run dev
```

ğŸ§ª Usage

Open the frontend in your browser.

Upload an X-ray image of a pipeline.

The image is sent to the FastAPI backend for processing.

The trained model predicts whether the pipeline is Defective or Normal.

The result is displayed instantly with a confidence score.

---

# ğŸ”¬ Model Architecture and Training Process

## âš™ï¸ **Key Features**
- ğŸ” **Defect Extraction:** Uses image masks to extract defect areas from the original images.
- ğŸ·ï¸ **Label Automation:** Automatically generates labels (Defect / No Defect) based on extracted image data.
- ğŸ¤– **Deep Learning Model:** A lightweight CNN built using PyTorch for binary classification of defects.
- ğŸ’¾ **Batch Processing:** Efficiently processes and labels large sets of training and test images.
- ğŸ“Š **Prediction Output:** Generates CSV reports with predicted defect statuses for easy review.

---

## ğŸ§‘â€ğŸ’» **Technologies Used**
- Python
- OpenCV
- Matplotlib
- PyTorch
- NumPy
- Pandas

---

## ğŸ—‚ï¸ **Project Workflow**

1. **Defect Extraction**  
   Original images and their corresponding ground truth masks are processed. The mask isolates the defective region, creating a clean dataset of defect patches.

2. **Label Generation**  
   Each extracted image is labeled as `1` (Defect Present) or `0` (No Defect) based on pixel inspection.

3. **Model Building**  
   A custom CNN is trained to classify images as defective or non-defective. The model uses convolutional layers for feature extraction and fully connected layers for classification.

4. **Model Training & Evaluation**  
   The model is trained on the labeled dataset, validated, and saved for later inference.

5. **Inference on Test Set**  
   The saved model is used to predict defects on new, unseen test images. The predictions are saved as a `.csv` file for review.
